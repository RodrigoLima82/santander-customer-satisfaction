{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Formação Cientista de Dados</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto 03 - Prevendo o Nível de Satisfação dos Clientes do Santander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook contém as seguintes fases para a análise dos modelos de Machine Learning\n",
    " - Pré-Processamento\n",
    " - Criação do Modelo de Machine Learning\n",
    " - Validação e Otimização do Modelo\n",
    " - Previsão e Relatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do Problema de Negócio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Banco Santander gostaria de identificar clientes insatisfeitos no início do relacionamento. Isso irá permitir que o Santander adote medidas proativas para melhorar a felicidade de um cliente antes que seja tarde demais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/c/santander-customer-satisfaction/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informações sobre os atributos:\n",
    "\n",
    "O dataset possui um grande número de variáveis numéricas e anonimas. A coluna 'TARGET' é a variável preditora\n",
    "\n",
    "0. Clientes Satisfeitos\n",
    "1. Clientes Insatisfeitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo e Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas que serao utilizadas neste projeto\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando arquivo csv usando Pandas\n",
    "train = pd.read_csv(\"../data/train.csv\", header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando o shape do dataset (76.020 linhas x 371 colunas)\n",
    "# Neste caso algoritmo pode apresentar problemas de performance devido a alta dimensionalidade.\n",
    "print(train.shape)\n",
    "\n",
    "# Visualizando as 20 primeiras linhas do dataset\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo de dados de cada atributo\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumário estatístico\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existe um problema de desbalanceamento de classes, ou seja, volume maior de um dos tipos de classe. \n",
    "# Podemos ver abaixo que existe uma clara desproporção (menos de 4% sao clientes insatisfeitos)\n",
    "# entre as classes 0 (clientes satisfeitos) e 1 (clientes insatisfeitos).\n",
    "\n",
    "# Visualizando a distribuição das classes (variavel TARGET)\n",
    "pd.value_counts(train['TARGET']).plot.bar()\n",
    "plt.title('TARGET histogram')\n",
    "plt.xlabel('TARGET')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Visualizando um df com quantidade e percentual da variavel TARGET\n",
    "df = pd.DataFrame(train['TARGET'].value_counts())\n",
    "df['%'] = 100*df['TARGET']/train.shape[0]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolvendo o problema de Overfitting da variavel TARGET utilizando o OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo a coluna ID do dataset\n",
    "train = train.drop(\"ID\", axis=1)\n",
    "\n",
    "# Visualizando as 20 primeiras linhas do dataset\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolvendo problema de Overfitting utilizando o OverSampling\n",
    "\n",
    "# Import dos módulos\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = train.iloc[:,:-1]\n",
    "Y = train.TARGET\n",
    "\n",
    "# Definindo o tamanho das amostras\n",
    "teste_size = 0.33\n",
    "\n",
    "# Criando os conjuntos de dados de treino e de teste\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = teste_size, random_state = 0)\n",
    "\n",
    "# Aplicando a funcao SMOTE\n",
    "# SMOTE eh um metodo de oversampling. Ele cria exemplos sinteticos da classe minoritaria ao inves de criar copias\n",
    "sm = SMOTE(random_state=0)\n",
    "X_treino_res, Y_treino_res = sm.fit_sample(X_treino, Y_treino.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição das classes (variavel TARGET) apos aplicar OverSampling\n",
    "pd.value_counts(Y_treino_res).plot.bar()\n",
    "plt.title('TARGET histogram')\n",
    "plt.xlabel('TARGET')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Visualizando um df com quantidade e percentual da variavel TARGET\n",
    "df = pd.DataFrame(pd.value_counts(Y_treino_res), columns=['TARGET'])\n",
    "df['%'] = 100*df['TARGET']/Y_treino_res.shape[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - Método Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagged Decision Trees, como o algoritmo RandomForest (esses são chamados de Métodos Ensemble), podem ser usados para estimar a importância de cada atributo. Esse método retorna um score para cada atributo.\n",
    "Quanto maior o score, maior a importância do atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importância do Atributo com o Extra Trees Classifier\n",
    "\n",
    "# Import dos Módulos\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = X_treino_res\n",
    "Y = Y_treino_res\n",
    "\n",
    "# Criação do Modelo - Feature Selection\n",
    "modelo = ExtraTreesClassifier()\n",
    "modelo.fit(X, Y)\n",
    "\n",
    "# Convertendo o resultado em um dataframe\n",
    "df = pd.DataFrame(train.columns,columns=['Coluna'])\n",
    "df['Importancia'] = pd.DataFrame(modelo.feature_importances_.astype(float))\n",
    "\n",
    "# Realizando a ordenacao por Importancia (Maior para Menor)\n",
    "result = df.sort_values('Importancia',ascending=False)\n",
    "\n",
    "# Imprimindo as 20 variaveis mais importantes\n",
    "cols_of_interest = result[1:20]['Coluna']\n",
    "cols_of_interest = cols_of_interest.append(pd.Series(['TARGET']))\n",
    "print(cols_of_interest)\n",
    "\n",
    "# Deixando somente as colunas de interesse no df de treino\n",
    "new_train = train[cols_of_interest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumário estatístico\n",
    "new_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se existem valores nulos no novo dataset\n",
    "new_train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "import seaborn as sns\n",
    "sns.pairplot(new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação e Validação dos Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = new_train.iloc[:,:-1]\n",
    "Y = new_train.TARGET\n",
    "\n",
    "# Padronizando os dados (0 para a média, 1 para o desvio padrão)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Definindo os valores para o número de folds\n",
    "num_folds = 10\n",
    "seed = 10\n",
    "\n",
    "# Preparando a lista de modelos\n",
    "modelos = []\n",
    "modelos.append(('LR', LogisticRegression()))\n",
    "modelos.append(('LDA', LinearDiscriminantAnalysis())) # LDA: 0.960024 (0.002729)\n",
    "modelos.append(('KNN', KNeighborsClassifier())) # KNN: 0.958958 (0.002947)\n",
    "modelos.append(('CART', DecisionTreeClassifier())) # CART: 0.928611 (0.002662)\n",
    "#modelos.append(('SVM', SVC())) # SVM: 0.960089 (0.002885)\n",
    "\n",
    "# Avaliando cada modelo em um loop\n",
    "resultados = []\n",
    "nomes = []\n",
    "\n",
    "for nome, modelo in modelos:\n",
    "    kfold = KFold(n_splits = num_folds, random_state = seed)\n",
    "    cv_results = cross_val_score(modelo, X, Y, cv = kfold, scoring = 'accuracy')\n",
    "    resultados.append(cv_results)\n",
    "    nomes.append(nome)\n",
    "    msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# Boxplot para comparar os algoritmos\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Comparação de Algoritmos de Regressao')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(resultados)\n",
    "ax.set_xticklabels(nomes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizando Performance com Métodos Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo XGBoost - Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos módulos\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "X = new_train.iloc[:,:-1]\n",
    "Y = new_train.TARGET\n",
    "\n",
    "# Padronizando os dados (0 para a média, 1 para o desvio padrão)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Definindo o tamanho dos dados de treino e de teste\n",
    "teste_size = 0.3\n",
    "seed = 10\n",
    "\n",
    "# Criando o dataset de treino e de teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, Y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modeloXGB = XGBClassifier()\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloXGB.fit(X, Y)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred = modeloXGB.predict(X_teste)\n",
    "previsoes = [round(value) for value in y_pred]\n",
    "\n",
    "# Avaliando as previsões\n",
    "accuracy = accuracy_score(y_teste, previsoes)\n",
    "print(\"Acurácia: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Curva ROC \n",
    "# A Curva ROC permite analisar a métrica AUC (Area Under the Curve).\n",
    "aucResult = cross_val_score(modeloXGB, X, Y, cv = kfold, scoring = 'roc_auc')\n",
    "print(\"AUC: %.3f\" % (aucResult.mean() * 100))\n",
    "\n",
    "# Confusion Matrix\n",
    "# Permite verificar a acurácia em um formato de tabela\n",
    "matrix = confusion_matrix(y_teste, previsoes)\n",
    "print(\"Confusion Matrix\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relatório de Classificação\n",
    "\n",
    "# Import dos módulos\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Fazendo as previsões e construindo o relatório\n",
    "report = classification_report(y_teste, previsoes)\n",
    "\n",
    "# Imprimindo o relatório\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criacao de função para criar um plot para a confusion matrix\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamando a função para visualizar a confusion matrix\n",
    "plot_confusion_matrix(matrix, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['high', 'low'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
